docker run -d --name els-01 --net pub_net --ip=172.21.1.31 --memory="800m" -p 9200:9200 -p 9300:9300 -v els-01-data:/usr/share/elasticsearch -v els-backup:/mnt/backup docker.elastic.co/elasticsearch/elasticsearch:7.13.0
docker run -d --name els-02 --net pub_net --ip=172.21.1.32 --memory="800m" -p 9200:9200 -p 9300:9300 -v els-02-data:/usr/share/elasticsearch -v els-backup:/mnt/backup docker.elastic.co/elasticsearch/elasticsearch:7.13.0

#delete data dir for reinit node
/var/lib/elasticsearch/nodes/0/_state

sysctl -w vm.max_map_count=262144
vi /etc/sysctl.conf
make entry vm.max_map_count=262144
restart

/etc/elasticsearch/jvm.options  -- set memory

curl -XGET 'http://localhost:9200/_cluster/health?pretty'

/etc/kibana/kibana.yml

node.name: els-02               # Имя ноды
node.roles: [ master, data ]  # Роли узла [ data, master, voting_only ]
#
# ---------------------------------- Network -----------------------------------
network.host: 172.21.1.32       # Адрес узла
http.port: 9200                                 # Порт

transport.tcp.port: 9300
#
# ---------------------------------- Cluster -----------------------------------
cluster.name: es_cluster                                             # Имя кластера
cluster.initial_master_nodes: ["els-01","els-02"]  # Начальный набор мастер узлов
#
# --------------------------------- Discovery ----------------------------------
discovery.seed_hosts: ["172.21.1.31", "172.21.1.32", "172.21.1.33"] # Узлы кластера
#
# ----------------------------------- Paths ------------------------------------
path.data: /usr/share/elasticsearch/data # Директория с данными
path.logs: /usr/share/elasticsearch/logs # Директория с логами
path.repo: /mnt/backup
#----------------------------------
index.number_of_shards: 3
index.number_of_replicas: 1

#---------------------------------


POST example-index-sharded/_split/nginx-filebeat
{
       "settings": {
               "index.number_of_shards": 3
       }
}

PUT nginx-filebeat/_settings
{
  "index.blocks.write": false
}


#add replica
PUT aminno_member_email/_settings
{
   "index" : {
       "number_of_replicas" : 1,
       "refresh_interval" : "1s"
   }
}

#node id
GET /_nodes

#query time and others
GET /_stats/search/


#check exclusion for master voting
GET /_cluster/state?filter_path=metadata.cluster_coordination.voting_config_exclusions

#add to exclusiuons on master voting
POST /_cluster/voting_config_exclusions?node_names=node_name&timeout=1m

#delete from eclusions on master voting
DELETE /_cluster/voting_config_exclusions?wait_for_removal=false

#elastic backup and restore
GET /_snapshot/_all

  #register new snapshot repository
PUT /_snapshot/my_fs_backup
{
  "type": "fs",
  "settings": {
    "location": "/mount/backups/my_fs_backup_location",
    "compress": true
  }
}

  #unregister
DELETE /_snapshot/my_fs_backup

  #verify
POST /_snapshot/backup1/_verify

   #clean
POST /_snapshot/my_repository/_cleanup

  #create snapshot
PUT /_snapshot/my_backup/snapshot_1?wait_for_completion=true

PUT /_snapshot/my_backup/snapshot_2?wait_for_completion=true
{
  "indices": "data_stream_1,index_1,index_2",
  "ignore_unavailable": true,
  "include_global_state": false,
  "metadata": {
    "taken_by": "kimchy",
    "taken_because": "backup before upgrading"
  }
}

   #get snapshots in repo
GET /_snapshot/backup1/*

   # monitor snapshot making
GET /_snapshot/my_backup/_current
GET /_snapshot/_status



   #restore
POST /_snapshot/my_backup/snapshot_1/_restore

POST /_snapshot/my_backup/snapshot_1/_restore
{
  "indices": "data_stream_1,index_1,index_2",
  "ignore_unavailable": true,
  "include_global_state": false,              
  "rename_pattern": "index_(.+)",
  "rename_replacement": "restored_index_$1",
  "include_aliases": false
}

  #restore with changing index settings
POST /_snapshot/my_backup/snapshot_1/_restore
{
  "indices": "index_1",
  "ignore_unavailable": true,
  "index_settings": {
    "index.number_of_replicas": 1
  },
  "ignore_index_settings": [
    "index.refresh_interval"
  ]
}


   #delete snapshot
DELETE /_snapshot/my_backup/snapshot_1


    # automatic snapshots policy

GET /_slm/policy


PUT /_slm/policy/nightly-snapshots
{
  "schedule": "0 30 1 * * ?", 
  "name": "<nightly-snap-{now/d}>", 
  "repository": "my_repository", 
  "config": { 
    "indices": ["*"] 
  },
  "retention": { 
    "expire_after": "30d", 
    "min_count": 5, 
    "max_count": 50 
  }
}

POST /_slm/policy/nightly-snapshots/_execute
<seconds> <minutes> <hours> <day_of_month> <month> <day_of_week> [year]

 #like maintenance mode/ decomission

PUT _cluster/settings
{
  "transient" : {
    "cluster.routing.allocation.exclude._ip" : "10.0.0.1"
  }
}
This will move all shards from 10.0.0.1 to other nodes (will take time depending on the data). Once everything is done, you can kill the node, perform maintenance and get it back online. This is a slower operation and is not required if you have replicas.


#disable\enable auto relocation and auto rebalancing
curl -XPUT "localhost:9200/_cluster/settings" -d '{
  "transient" : {
     "cluster.routing.allocation.enable" : "none"
     #"cluster.routing.allocation.enable" : "all"
  }
}'


#текущие настройки 
_cluster/settings?include_defaults=true&pretty